3.4.3数据预处理

数据值特别大或特别小时，数据高度不对称，算法无法处理所有不在同一纬度的数据，最终可能会导致网络模型训练失败，数据预处理是为了使数据无偏低方差
0均值（Zero Centralization)
归一化(Normalization)
主成分分析（Principal Component Analysis)
白化（Whitening)

0均值

归一化
第一种，将0均值后数据的每一维除以每一维的标准差（相对残差？）
第二种，数据中的每一纬度归一化到区间【a，b】。这种只适用于数据的不同维度应该具有相同的重要性时，也就是每一维数据的权重一样时。
例如数组np.array([1024,222,0.0216,0.0412,19566]),当输入的数据大小参差不齐时，有必要对数据进行归一化操作后再乘以该维度数据的权重值。
对于图像来说rgb图像的像素区间一般固定，所以不用归一化处理。

pca
用来寻找有效表示数据主轴的方向，减少计算量,降低数据噪声，使得神经网络，svm等分类器效果更好
主成分分析的新特征最大化样本方差。从旧特征到新特征的映射捕获数据中的固有变异性
  协方差矩阵（Covariance Matrix):
  cov(x,y) = Σ(i=1,n)(xi-x均)(yi-y均)]/(n-1)
  协方差矩阵对角线上的元素表示数据的方差，假设协方差矩阵为M，那么M（i，j）（i!=j)表示第i维和第j维数据的特征相关性
  奇异值分解（Signular Value Decomposition,SVD)
  A = USV'
  矩阵奇异值分解可以将一个复杂的矩阵用更小更简单的3个子矩阵的相乘来表示，这些小矩阵描述矩阵的重要特性。
  对矩阵进行奇异值分解，得到3个矩阵（U酉矩阵，S对角矩阵，V'共轭转置矩阵）
  假设A是一个N*M的矩阵，那么得到的酉矩阵U是个N*N的矩阵（U中的向量称为左奇异向量，列为特征向量）
  对角矩阵S是一个N*M的矩阵（除对角线外其余元素为0，对角线上元素称为奇异值），共轭转置矩阵V'是一个M*M的矩阵（V中的向量称为右奇异向量）
  为了对数据去相关性，需要把原始的数据投影到特征向量上。SVD分解后得到的S矩阵的列是相互正交的向量，因此他们可以被看成基向量，用于去数据相关性。
  这种投影相当于将数据旋转并投影到新的基向量轴上。
在对数据进行主成分分析之前不能直接求数据的协方差矩阵，首先需要把数据经过0均值处理，然后才能计算其协方差矩阵，得到数据不同维度之间的相关性。
接着可以对协方差矩阵进行SVD分解，得到3个矩阵。
SVD分解一个很好的特性，因为返回的U矩阵式按照其特征值的大小排序的，排在前面的就是主方向，因此可以通过选取前几个特征向量来降低数据的维度。
这里就是主成分分析的降维方法。
经过上述操作后，就能将原始数据从n=N*D维转变成m=N*100，N*100的矩阵中保留原始数据的最大方差。（且数据会以坐标原点为中心）

白化
目的是降低数据的冗余性，希望 使数据具有如下性质
1.特征之间相关性较低
2.所有特征具有相同的方差
（且每一维的数据根据特征值进行了缩放且符合高斯分布）
方法：把通过pca的数据从对角矩阵再变成单位矩阵，使得数据具有相同的方差。
缺点：在于扩大数据的噪声，因为它将数据的维度拉升到相同 的大小，把对角矩阵变成单位矩阵
数据白化实现时要防止矩阵元素除以0
Xwhiten = PCA / np.sqrt(S + 1e-6)#除以特则会那个之，也就是奇异值的平方根

需要注意，图像处理中，很少用pca和白化，因为两者是为了对数据进行完整性操作，图像中的像素值已经包含了丰富且相关联的完整信息。
但是对图像进行0均值是非常重要的。因此在任何一个深度学习的例子中输入数据都会进行0均值操作。而归一化也是很常见的数据预处理操作，其负责数据的一致性

