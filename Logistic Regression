Is an algorithm for binary classfication(用于二分分类）（是或不是）
can get a chance（概率） about what you are predicting
例如线性回归得到的是一个值，但不一定介于0，1之间，所以不适合用于得到一个概率，但logistic不一样，引进sigmoid函数后可以得到一个介于0，1的。
其实，sigmoid（w'x+b)就相当于对线性回归的表达式进行区间限制。（个人理解）
b相当于拦截器，即《图解神经网络》中的阈值。

Loss（error） function:即《图解神经网络》中的误差(error)函数。
可以用残差平方或其0.5作为loss function ，但实践表明这会使之后讨论的优化问题变成非凸的，即多个局部最优解，使得使用梯度下降法时可能找不到全局最优解。
故构造如下误差函数
respect to a single training example:（衡量在单个训练样本时的表现）
Loss(error) function:
L(y',y)=-(y*ln(y')+(1-y)*ln(1-y'))
if y=1 :y'->1 , L->0  
if y=0 :y'->0 , L->0
y' = sigmoid(x), 0<y'<1

Measures how well you're doing an entire training set:(衡量在全体训练样本上的表现）
Cost funcion:
J(w,b) = 1/m*∑L(y'(i),y(i))求均值

算法示例伪代码：
J = 0;dw1=0;dw2=0;db=0
For i=1 to m
  z(i) = w'x(i)+b
  a(i) = sigma(z(i))
  J+=-[y(i)ln(a(i))+(1-y(i))ln(1-a(i))]
  dz(i) = a(i)-y(i)
  dw1+=x1(i)*dz(i)  //这里dw1没有标号，即作为累加器，便于循环遍历之后平均
  dw2+=x2(i)*dz(i)
  db += dz(i)
J/=m
dw1/=m;dw2/=m;db/=m.

w1:=w1-alpha*dw1
w2:=w2-alpha*dw2
b:=b-alpha*db

Logistic表示概率的理由莫不是：大值的概率小，小值的概率小，类似正太随机分布?https://www.cnblogs.com/happylion/p/4169945.html
假设y=1的概率为y',则y=0的概率为1-y'，互斥（独立同分布）
也可y=0的概率为y',则y=1的概率为1-y'，这是可以任意的。只是训练得到的参数会改变
所以x的预测会有两个输出y = 0,y = 1两个事件的概率，但我们都只算y=1的概率，然后再用1-y'得到另一个的概率。

#浅层神经网络
所有参数中，W会存在如果多个神经元的参数相同，则自始自终的计算是相同的，从而失去多神经元的意义，b则不存在这个对称性问题
初始化的w参数小些是为了避免开始就使得线性和落在了导数近乎0的地方，
线性和开始时接近0，那么开始的变化幅度就比较大（梯度下降比较快）这个问题主要针对存在tanh类函数时
